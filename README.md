# Airflow_DAG-ETL
Setup workflow to perform ETL of S3 website endpoint.

# Introduction & Goals
Write a DAG ( Dynamic Acyclic Graph ) to orchestate the workflow of below tasks on Airflow.
   1. Capture S3 website endpoint GET response and write to S3 as JSON file.
   2. Transform the response and write to S3 as CSV

# Contents

- [The Data Set](#the-data-set)
- [Used Tools](#used-tools)
- [Apache Airflow](#Airflow)
  - DAG
      - [ETL](DAGs/asset_details_hook.py)
      - [Self-Check](DAGs/file_check.py)
- [Follow Me On](#follow-me-on)
- [Appendix](#appendix)


# The Data Set
### Explain the data set

The JSON response from S3 Website enpoints.


# Used Tools
- Client

Jupyter NoteBook, VS Code

- Wokflow Tool

Apache Airflow

- Programming

Python


# Airflow
   - [Setup](https://github.com/vijaykothareddy/Data-Engineering/blob/master/Blogs_Code/Airflow_Configuration.MD)

# Follow Me On
Add the link to your LinkedIn Profile


